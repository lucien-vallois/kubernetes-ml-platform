# Default values for ml-service
# This is a YAML-formatted file.

# Global configuration
global:
  imageRegistry: ""
  imagePullSecrets: []
  storageClass: "gp2"

# ML Training Service
training:
  enabled: true
  image:
    repository: ml-platform/training
    tag: "latest"
    pullPolicy: IfNotPresent

  # Resource requests and limits
  resources:
    requests:
      cpu: "2"
      memory: "8Gi"
      nvidia.com/gpu: "1"
    limits:
      cpu: "8"
      memory: "32Gi"
      nvidia.com/gpu: "1"

  # Training configuration
  config:
    backend: "horovod"
    batchSize: 32
    learningRate: 0.001
    maxEpochs: 100
    checkpointDir: "/checkpoints"
    modelDir: "/models"

  # Autoscaling
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
    targetGPUUtilizationPercentage: 80

  # Persistence
  persistence:
    enabled: true
    size: 100Gi
    accessMode: ReadWriteMany

# ML Inference Service
inference:
  enabled: true
  image:
    repository: ml-platform/inference
    tag: "latest"
    pullPolicy: IfNotPresent

  replicas: 3

  resources:
    requests:
      cpu: "1"
      memory: "4Gi"
    limits:
      cpu: "4"
      memory: "16Gi"

  # Inference configuration
  config:
    modelPath: "/models/best_model.pt"
    batchSize: 64
    numWorkers: 4
    timeout: 30

  # Service configuration
  service:
    type: ClusterIP
    port: 8080
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "8080"

  # Autoscaling
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70

  # Ingress
  ingress:
    enabled: false
    className: ""
    annotations: {}
    hosts:
      - host: ml-inference.local
        paths:
          - path: /
            pathType: Prefix
    tls: []

# GPU Support
gpu:
  enabled: true
  devicePlugin:
    image:
      repository: nvidia/k8s-device-plugin
      tag: "v0.12.3"

# Monitoring
monitoring:
  enabled: true

  prometheus:
    enabled: true
    server:
      persistentVolume:
        enabled: true
        size: 50Gi
      retention: "30d"

  grafana:
    enabled: true
    adminPassword: "admin"
    persistence:
      enabled: true
      size: 10Gi

  serviceMonitor:
    enabled: true
    interval: 30s
    scrapeTimeout: 10s

# Storage
storage:
  s3:
    enabled: true
    bucket: "ml-platform-artifacts"
    region: "us-east-1"

  efs:
    enabled: true
    filesystemId: ""

# Security
security:
  serviceAccount:
    create: true
    name: "ml-service-account"
    annotations:
      eks.amazonaws.com/role-arn: ""

  podSecurityContext:
    fsGroup: 1000

  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL

# Node selectors and tolerations
nodeSelector: {}

tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"

affinity:
  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      preference:
        matchExpressions:
        - key: node-type
          operator: In
          values:
          - gpu

# Additional environment variables
extraEnvVars: []

# Additional volumes
extraVolumes: []

# Additional volume mounts
extraVolumeMounts: []
